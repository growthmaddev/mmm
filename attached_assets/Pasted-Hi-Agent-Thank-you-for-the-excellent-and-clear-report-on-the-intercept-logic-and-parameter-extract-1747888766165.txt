Hi Agent,

Thank you for the excellent and clear report on the intercept logic and parameter extraction for model 26.

**Confirmation:**
* The intercept logic in `train_mmm.py` (using the actual learned intercept scaled by `len(df)`, with warnings but no arbitrary adjustments to a target percentage) is now correctly implemented and approved.
* Your analysis of model 26 confirms that the R-squared (~3.4%) and baseline (~36 total) remain very low. Crucially, it reveals that the model is heavily relying on **default values for adstock parameters and for saturation parameters L and k**, while only `x0` seems data-driven. This lack of flexibility in learning channel-specific transformations is a prime suspect for the poor model fit.

We need to act on your previous excellent suggestions to improve the model. Please proceed with the following modifications to `train_mmm.py`:

**Task 1: Implement Data-Driven Adstock & Saturation Parameters**
    a.  Referencing your prior suggestion for "Adaptive Saturation Parameters," implement logic to make adstock and saturation parameters more data-driven for each channel *before* model fitting. For example:
        i.  **Saturation `x0` (Midpoint):** Ensure `x0` is set based on the median spend (or another quantile) for each specific channel (as it seems to be partially doing).
        ii. **Saturation `k` (Steepness):** Implement your suggestion to adjust `k` based on data scale (e.g., inversely to spend level: `k = 0.0005 / (x0 / 1000 if x0 > 0 else 1)` or a similar adaptive approach).
        iii. **Saturation `L` (Ceiling):** While often fixed at 1.0 for normalized data, consider if `L` should also be made adaptable or if the current default is appropriate given other potential data scaling. For now, focus on `x0` and `k`.
        iv. **Adstock `alpha` (Decay) & `l_max` (Max Lag):** These are currently hardcoded defaults (`alpha=0.3, l_max=3`). Explore if PyMC-Marketing allows these to be learned (i.e., set as priors in the model rather than fixed values) for each channel. If direct learning is too complex for a first step, could you set more informed, data-driven *initial values or bounds* for these per channel, rather than a single global default? For now, at least make the `l_max` potentially configurable per channel if the data suggests different lag effects.
    b.  These more adaptive parameters should then be used when initializing the `MMM` object (e.g., via the `media_transforms` argument as you previously sketched).

**Task 2: Implement Initial Data Preprocessing Options**
    a.  **Log-Transform Target Variable:** Implement the option to log-transform the target variable `y` (e.g., `y_transformed = np.log1p(y)`). This should be configurable (e.g., via the input `config` JSON).
        * Remember that if the target is transformed, predictions from the model will be on the transformed scale and will need to be back-transformed to the original scale before calculating metrics like RMSE against the original `y` or for reporting sales.
    b.  **Scale Predictor Variables:** Implement the option to standardize/scale the predictor variables (channel spends and any control variables) using `sklearn.preprocessing.StandardScaler`. This should also be configurable. The scaler object will need to be saved if new data needs to be transformed for prediction later (though this is for future consideration, for now, focus on training).

**Task 3: Implement Data Diagnostic Checks (High Priority)**
    a.  Implement the `run_diagnostics` function as previously detailed (checking data volume, spend-sales correlations, collinearity via PCA, and target variable stationarity via ADF test if a date column is present).
    b.  Integrate the output of this function into a new top-level key (e.g., `"data_diagnostics_report"`) in the JSON results from `train_mmm.py`.
    c.  Ensure warnings from diagnostics are also printed to `stderr`.

**MCMC Settings:** For these development iterations, you can keep the MCMC settings at faster values (e.g., `draws=200, tune=100`). We will use user-run tests with higher settings to evaluate the impact of these changes on final model performance.

**Deliverables:**
1.  The updated `train_mmm.py` script incorporating these changes (adaptive adstock/saturation, configurable target transform and predictor scaling, and the data diagnostics function and output).
2.  An example of the new `"data_diagnostics_report"` section from a test run.
3.  A brief summary of how adstock/saturation parameters are now being handled.

These changes are focused on making the model more flexible and data-responsive, and on giving us better tools to understand the data itself.